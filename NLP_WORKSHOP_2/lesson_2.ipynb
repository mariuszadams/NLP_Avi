{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Financial Sentiment Analysis\n",
    "\n",
    "This notebook is a step-by-step review of a text preprocessing functions used as a initial step of sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import os\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory input data analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/FinancialNews.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp_workshop/lib/python3.9/site-packages/pandas/io/parquet.py:651\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    500\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    654\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp_workshop/lib/python3.9/site-packages/pandas/io/parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     65\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(\"data/FinancialNews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences examples\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "data.iloc[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment feature - number of occurences\n",
    "data[\"Sentiment\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique chars\n",
    "unique_chars = pd.Series(\n",
    "    [char for sentence in data[\"Sentence\"] for char in sentence]\n",
    ").unique()\n",
    "print(\"Number of unique chars:\", len(unique_chars))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans input text by lowercasing and removing punctuation.\n",
    "\n",
    "    :param text: An input string to be cleaned\n",
    "    :return: Cleaned string\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    print(\"Lower case\")\n",
    "    print(text)\n",
    "\n",
    "    text = re.compile(r\"https?://\\S+|www\\.\\S+\").sub(\"\", text)\n",
    "\n",
    "    print()\n",
    "    print(\"Links\")\n",
    "    print(text)\n",
    "\n",
    "    desired_elements = r\"[^a-z\\?\\!\\'\\ ]\"\n",
    "    text = re.sub(desired_elements, \"\", text)\n",
    "\n",
    "    print()\n",
    "    print(\"Desired signs\")\n",
    "    print(text)\n",
    "\n",
    "    text = \" \".join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "    print()\n",
    "    print(\"Contractions\")\n",
    "    print(text)\n",
    "\n",
    "    replacements = {\n",
    "        r\"'s\\b\": \"\",\n",
    "        r\"\\s+\": \" \",\n",
    "    }\n",
    "\n",
    "    for replace, by in replacements.items():\n",
    "        text = re.sub(replace, by, text)\n",
    "\n",
    "    print()\n",
    "    print(\"Space and 's\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sentence = \"The company plans to increase the unit's specialist staff to several dozen -- it's going to depend on the market situation during 2010 . We're happy for that. Check https://t.co/jNDphllzq5 for more!\"\n",
    "example_sentence = clean_text(initial_sentence)\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Sentence\"][0])\n",
    "print()\n",
    "clean_text(data[\"Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Sentence\"][4])\n",
    "print()\n",
    "clean_text(data[\"Sentence\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"language_detector\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector()  # Create the detector component\n",
    "\n",
    "\n",
    "# Add language detector component\n",
    "nlp.add_pipe(\"language_detector\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str, nlp):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by splitting it into words.\n",
    "\n",
    "    :param text: An input string to be tokenized\n",
    "    :param nlp: A SpaCy model\n",
    "    :return: An array of tokens\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenize_text(example_sentence, nlp)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_language(text):\n",
    "    \"\"\"\n",
    "    Check the language of the given text.\n",
    "\n",
    "    :param text: Text to check the language.\n",
    "    :return: Language of the text.\n",
    "    \"\"\"\n",
    "    return text._.language[\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"how\",\n",
    "    \"in\",\n",
    "    \"on\",\n",
    "    \"or\",\n",
    "    \"the\",\n",
    "    \"what\",\n",
    "    \"will\",\n",
    "]\n",
    "\n",
    "\n",
    "def remove_stop_words(tokenized_words, stop_words: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes the stop-words from the list of tokenized words.\n",
    "\n",
    "    :param tokenized_words: Array of words after tokenization\n",
    "    :return: Array of words after stop-words removing\n",
    "    \"\"\"\n",
    "    words_removed_stop_words = [\n",
    "        word for word in tokenized_words if word.text not in stop_words\n",
    "    ]\n",
    "\n",
    "    return words_removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = remove_stop_words(doc, stop_words)\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokenized_words):\n",
    "    \"\"\"\n",
    "    Lemmatizes words from the list of tokenized words.\n",
    "\n",
    "    :param tokenized_words: List of words after tokenization\n",
    "    :return: List of words after lemmatization\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatized_words = [token.lemma_ for token in tokenized_words]\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_words(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text function but without internal print functions\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans input text by lowercasing and removing punctuation.\n",
    "\n",
    "    :param text: An input string to be cleaned\n",
    "    :return: Cleaned string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.compile(r\"https?://\\S+|www\\.\\S+\").sub(\"\", text)\n",
    "\n",
    "    desired_elements = r\"[^a-z\\?\\!\\'\\ ]\"\n",
    "    text = re.sub(desired_elements, \"\", text)\n",
    "\n",
    "    text = \" \".join([contractions.fix(word) for word in text.split()])\n",
    "\n",
    "    replacements = {\n",
    "        r\"'s\\b\": \"\",\n",
    "        r\"\\s+\": \" \",\n",
    "    }\n",
    "    for replace, by in replacements.items():\n",
    "        text = re.sub(replace, by, text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_on_sentence(sentence: str, stop_words: list, nlp):\n",
    "    \"\"\"\n",
    "    Performs the whole preprocessing procedure on the given sentence.\n",
    "\n",
    "    :param sentence: Sentence we want to preprocess.\n",
    "    :param stop_words: List with stop-words we want to remove from the sentence.\n",
    "    :param nlp: SpaCy model.\n",
    "\n",
    "    :return str: Sentence after preprocessing.\n",
    "    \"\"\"\n",
    "    preprocessed_sentence = clean_text(sentence)\n",
    "    preprocessed_sentence = tokenize_text(preprocessed_sentence, nlp)\n",
    "    preprocessed_sentence = remove_stop_words(preprocessed_sentence, stop_words)\n",
    "    preprocessed_sentence = lemmatize_words(preprocessed_sentence)\n",
    "    preprocessed_sentence = \" \".join(preprocessed_sentence)\n",
    "\n",
    "    return preprocessed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessing_on_sentence(initial_sentence, stop_words, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_on_dataframe(\n",
    "    dataframe: pd.DataFrame, stop_words: list, nlp, save_to_csv=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform the whole preprocessing procedure on the Pandas DataFrame\n",
    "    with columns \"Sentence\" and \"Sentiment\".\n",
    "\n",
    "    :param dataframe: Pandas Dataframe containing \"Sentence\" and \"Sentiment\" columns.\n",
    "    :param stop_words: List with stop-words we want to remove from sentences.\n",
    "    :param nlp: SpaCy model.\n",
    "    :param save_to_parquet: when not None, saves the dataframe in a .parquet file\n",
    "                            with given name.\n",
    "\n",
    "    :return: Pandas Dataframe with \"PreprocessedSentence\" column added.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe[\"PreprocessedSentence\"] = dataframe[\"Sentence\"].apply(\n",
    "        lambda text: clean_text(text)\n",
    "    )\n",
    "    dataframe[\"PreprocessedSentence\"] = dataframe[\"PreprocessedSentence\"].apply(\n",
    "        lambda text: tokenize_text(text, nlp)\n",
    "    )\n",
    "\n",
    "    dataframe[\"Language\"] = dataframe[\"PreprocessedSentence\"].apply(\n",
    "        lambda text: check_language(text)\n",
    "    )\n",
    "\n",
    "    dataframe[\"PreprocessedSentence\"] = dataframe[\"PreprocessedSentence\"].apply(\n",
    "        lambda text: remove_stop_words(text, stop_words)\n",
    "    )\n",
    "    dataframe[\"PreprocessedSentence\"] = dataframe[\"PreprocessedSentence\"].apply(\n",
    "        lambda text: lemmatize_words(text)\n",
    "    )\n",
    "    dataframe[\"PreprocessedSentence\"] = dataframe[\"PreprocessedSentence\"].apply(\n",
    "        lambda text: \" \".join(text)\n",
    "    )\n",
    "\n",
    "    dataframe = dataframe[dataframe[\"Language\"] == \"en\"]\n",
    "\n",
    "    dataframe = dataframe[[\"Sentence\", \"PreprocessedSentence\", \"Sentiment\"]]\n",
    "\n",
    "    if save_to_csv is not None:\n",
    "        dataframe.to_csv(f\"data/{save_to_csv}\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = text_preprocessing_on_dataframe(\n",
    "    data, stop_words, nlp, save_to_csv=\"FinancialNewsPreprocessed.csv\"\n",
    ")\n",
    "preprocessed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_workshop",
   "language": "python",
   "name": "nlp_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
